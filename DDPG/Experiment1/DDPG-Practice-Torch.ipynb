{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the `render_mode` parameter to show the attempts of the agent in a pop up window.\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition as a namedtuple: a simple data structure to group together s_t, a_t, r, s_t+1\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "# Cyclic buffer: holds the recent transitions + sampling a random batch of transitions for training\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity=10_000):\n",
    "        # deque is used to automatically discard the oldest data\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    # Save a new transition in the replay buffer\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    # Randomly sample a batch of transitions from the replay buffer to train the agent\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    # Current size of the replay buffer\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from the Ornstein-Uhlenbeck process\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * torch.sqrt(torch.tensor(self.dt)) * torch.randn_like(self.mean)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = torch.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_states, upper_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_states, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        \n",
    "        # Initialize weights between -3e-3 and 3e-3 for all layers\n",
    "        self.fc1.weight.data.uniform_(-0.003, 0.003)\n",
    "        self.fc2.weight.data.uniform_(-0.003, 0.003)\n",
    "        self.fc3.weight.data.uniform_(-0.003, 0.003)\n",
    "        \n",
    "        # Initialize biases to zero for all layers\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.bias.data.zero_()\n",
    "        \n",
    "        self.upper_bound = upper_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Scale output to match the upper bound\n",
    "        x = x * self.upper_bound\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super(Critic, self).__init__()\n",
    "        # State pathway\n",
    "        self.state_fc1 = nn.Linear(num_states, 16)\n",
    "        self.state_fc2 = nn.Linear(16, 32)\n",
    "        \n",
    "        # Action pathway\n",
    "        self.action_fc1 = nn.Linear(num_actions, 32)\n",
    "        \n",
    "        # Combined pathway\n",
    "        self.fc1 = nn.Linear(32 + 32, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # State pathway\n",
    "        state_out = F.relu(self.state_fc1(state))\n",
    "        state_out = F.relu(self.state_fc2(state_out))\n",
    "        \n",
    "        # Action pathway\n",
    "        action_out = F.relu(self.action_fc1(action))\n",
    "        \n",
    "        # Concatenate state and action pathways\n",
    "        concat = torch.cat([state_out, action_out], dim=1)\n",
    "        \n",
    "        # Combined pathway\n",
    "        out = F.relu(self.fc1(concat))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer Memory\n",
    "CAPACITY = 20\n",
    "BATCH_SIZE = 8\n",
    "buffer_memory = ReplayBuffer(capacity=CAPACITY)\n",
    "\n",
    "# Assuming get_actor and get_critic return PyTorch models\n",
    "actor_model = Actor(num_states=num_states, upper_bound=upper_bound)\n",
    "critic_model = Critic(num_states=num_states, num_actions=num_actions)\n",
    "\n",
    "target_actor = Actor(num_states=num_states, upper_bound=upper_bound)\n",
    "target_critic = Critic(num_states=num_states, num_actions=num_actions)\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.load_state_dict(actor_model.state_dict())\n",
    "target_critic.load_state_dict(critic_model.state_dict())\n",
    "\n",
    "# Optimizers\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = optim.Adam(critic_model.parameters(), lr=critic_lr)\n",
    "actor_optimizer = optim.Adam(actor_model.parameters(), lr=actor_lr)\n",
    "\n",
    "# Discount Factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=torch.zeros(1), std_deviation=float(std_dev) * torch.ones(1))\n",
    "\n",
    "TAU = 0.005\n",
    "\n",
    "def train():\n",
    "    \n",
    "    if len(buffer_memory)<BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Ensure models and optimizers are set to training mode\n",
    "    target_actor.train()\n",
    "    target_critic.train()\n",
    "    critic_model.train()\n",
    "    actor_model.train()\n",
    "    \n",
    "    # Sample a batch of transitions from the buffer memory\n",
    "    transitions = buffer_memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Transpose the sampled batch to separate its components (state, action, reward, next_state) \n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Concatenate state, action, reward, and next_state batches into separate tensors\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    \n",
    "    # Update Critic Model\n",
    "    critic_optimizer.zero_grad()  # Reset gradients accumulation\n",
    "    with torch.no_grad():\n",
    "        target_actions = target_actor(next_state_batch)\n",
    "        y = reward_batch + GAMMA * target_critic(next_state_batch, target_actions)\n",
    "    critic_value = critic_model(state_batch, action_batch.unsqueeze(1))\n",
    "    critic_loss = torch.mean((y - critic_value) ** 2)\n",
    "    critic_loss.backward()  # Compute gradients\n",
    "    critic_optimizer.step()  # Apply gradients\n",
    "\n",
    "    # Update Actor Model\n",
    "    actor_optimizer.zero_grad()  # Reset gradients accumulation\n",
    "    actions = actor_model(state_batch)\n",
    "    critic_value = critic_model(state_batch, actions)\n",
    "    actor_loss = -torch.mean(critic_value)\n",
    "    actor_loss.backward()  # Compute gradients\n",
    "    actor_optimizer.step()  # Apply gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(target, original, tau):\n",
    "    target_state_dict = target.state_dict()\n",
    "    original_state_dict = original.state_dict()\n",
    "\n",
    "    for key in target_state_dict:\n",
    "        target_state_dict[key] = tau * original_state_dict[key] + (1 - tau) * target_state_dict[key]\n",
    "\n",
    "    target.load_state_dict(target_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, actor_model, noise_object, lower_bound, upper_bound):\n",
    "    \n",
    "    # Get action from the actor model\n",
    "    sampled_actions = actor_model(state).squeeze()\n",
    "\n",
    "    # Add noise to the action\n",
    "    noise = noise_object()\n",
    "    sampled_actions = sampled_actions + torch.tensor(noise, dtype=torch.float)\n",
    "\n",
    "    # Ensure action is within bounds\n",
    "    legal_action = torch.clamp(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return legal_action.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps = 200\n",
    "num_episodes = 100\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Pre-allocations\n",
    "reward_per_time_step = np.zeros([num_episodes, num_time_steps])\n",
    "average_reward_per_episode = np.zeros([num_episodes])\n",
    "x_coordinate_per_time_step = np.zeros([num_episodes, num_time_steps])\n",
    "y_coordinate_per_time_step = np.zeros([num_episodes, num_time_steps])\n",
    "angular_velocity_per_time_step = np.zeros([num_episodes, num_time_steps])\n",
    "action_per_time_step = np.zeros([num_episodes, num_time_steps])\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episodic_reward = 0\n",
    "    # Convert the state to a PyTorch tensor and add a batch dimension\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    time_step = 0\n",
    "    \n",
    "    while True:\n",
    "        # Store the state values\n",
    "        x_coordinate_per_time_step[ep, time_step] = state[0][0].numpy()\n",
    "        y_coordinate_per_time_step[ep, time_step] = state[0][1].numpy()\n",
    "        angular_velocity_per_time_step[ep, time_step] = state[0][2].numpy()\n",
    "        \n",
    "        # Assuming policy is a function that takes a state and an OUActionNoise instance and returns an action\n",
    "        action = policy(state_tensor, actor_model, ou_noise, lower_bound, upper_bound)\n",
    "\n",
    "        # Perform the action in the environment\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "        next_state_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Record the transition in the replay buffer\n",
    "        # Assuming buffer has a method 'push' that takes a transition tuple\n",
    "        buffer_memory.push(state_tensor, action_tensor, reward_tensor, next_state_tensor)\n",
    "\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        reward_per_time_step[ep, time_step] = reward\n",
    "\n",
    "        train()\n",
    "\n",
    "        update_target(target_actor, actor_model, TAU)\n",
    "        update_target(target_critic, critic_model, TAU)\n",
    "\n",
    "        # End this episode when `done` or `truncated` is True\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "        state_tensor = next_state_tensor\n",
    "        time_step += 1\n",
    "\n",
    "    average_reward_per_episode[ep] = np.mean(reward_per_time_step[ep, :])\n",
    "    \n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    \n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(f\"Episode * {ep} * Avg Reward is ==> {avg_reward}\")\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Episodic Reward\")\n",
    "plt.show()\n",
    "\n",
    "np.save('reward_per_time_step.npy', reward_per_time_step)\n",
    "np.save('average_reward_per_episode.npy', average_reward_per_episode)\n",
    "np.save('x_coordinate_per_time_step.npy', x_coordinate_per_time_step)\n",
    "np.save('y_coordinate_per_time_step.npy', y_coordinate_per_time_step)\n",
    "np.save('angular_velocity_per_time_step.npy', angular_velocity_per_time_step)\n",
    "np.save('action_per_time_step.npy', action_per_time_step)\n",
    "np.save('num_time_steps.npy', num_time_steps)\n",
    "np.save('num_episodes.npy', num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(capacity=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/zjn_yl050qn4kdvrggtg30sh0000gn/T/ipykernel_70698/4142535470.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sampled_actions = sampled_actions + torch.tensor(noise, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming env is your environment instance and buffer is an instance of ReplayBuffer\n",
    "state, _ = env.reset()\n",
    "episodic_reward = 0\n",
    "\n",
    "# Convert the state to a PyTorch tensor and add a batch dimension\n",
    "state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Assuming policy is a function that takes a state and an OUActionNoise instance and returns an action\n",
    "action = policy(state_tensor, actor_model, ou_noise, lower_bound, upper_bound)\n",
    "\n",
    "# Perform the action in the environment\n",
    "observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "# Record the transition in the replay buffer\n",
    "# Assuming buffer has a method 'push' that takes a transition tuple\n",
    "buffer_memory.push(state_tensor, torch.tensor(action), torch.tensor([reward], dtype=torch.float32), torch.tensor(observation, dtype=torch.float32).unsqueeze(0))\n",
    "len(buffer_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = buffer_memory.sample(8)\n",
    "batch = Transition(*zip(*transitions))\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)\n",
    "next_state_batch = torch.cat(batch.next_state)\n",
    "q_values = actor_model(state_batch)\n",
    "with torch.no_grad():\n",
    "    target_actions = target_actor(next_state_batch)\n",
    "    y = reward_batch + GAMMA * target_critic(next_state_batch, target_actions)\n",
    "\n",
    "critic_value = critic_model(state_batch, action_batch.unsqueeze(1))\n",
    "actions = actor_model(state_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/zjn_yl050qn4kdvrggtg30sh0000gn/T/ipykernel_22062/477118677.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sampled_actions = sampled_actions + torch.tensor(noise, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "torch_prev_state = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "action = policy(torch_prev_state, actor_model, ou_noise, lower_bound, upper_bound)\n",
    "observation, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "next_state_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Record the transition in the replay buffer\n",
    "# Assuming buffer has a method 'push' that takes a transition tuple\n",
    "buffer.push(torch_prev_state, action_tensor, reward_tensor, next_state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
